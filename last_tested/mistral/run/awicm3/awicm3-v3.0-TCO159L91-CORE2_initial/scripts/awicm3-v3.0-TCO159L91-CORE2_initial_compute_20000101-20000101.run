#!/bin/bash
#SBATCH --partition=compute
#SBATCH --time=00:20:00
#SBATCH --nodes=13
#SBATCH --output=/work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log/awicm3-v3.0-TCO159L91-CORE2_initial_awicm3_compute_20000101-20000101_%j.log --error=/work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log/awicm3-v3.0-TCO159L91-CORE2_initial_awicm3_compute_20000101-20000101_%j.log
#SBATCH --job-name=awicm3-v3.0-TCO159L91-CORE2_initial
#SBATCH --account=ab0995
#SBATCH --mail-type=NONE
#SBATCH --mem=0
#SBATCH --exclusive

module purge
module unload netcdf_c
module unload intel intelmpi
module load python3/2021.01-gcc-9.1.0
module load cmake/3.13.3
module load autoconf/2.69
module load nco
module load cdo
module load gcc/4.8.2
module unload intel intelmpi
module load intel/18.0.4 intelmpi/2018.5.288
module load libtool/2.4.6
module load automake/1.14.1

export LC_ALL=en_US.UTF-8
export FC=mpiifort
export F77=mpiifort
export MPIFC=mpiifort
export CC=mpiicc
export CXX=mpiicpc
export MPIROOT="$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
export MPI_LIB="$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"
export HDF5ROOT=/sw/rhel6-x64/hdf5/hdf5-1.8.14-parallel-impi-intel14/
export HDF5_C_INCLUDE_DIRECTORIES=$HDF5ROOT/include
export HDF5_ROOT=$HDF5ROOT
export NETCDFFROOT=/sw/rhel6-x64/netcdf/netcdf_fortran-4.4.2-parallel-impi-intel14/
export NETCDFROOT=/sw/rhel6-x64/netcdf/netcdf_c-4.3.2-parallel-impi-intel14/
export NETCDF_Fortran_INCLUDE_DIRECTORIES=$NETCDFFROOT/include
export NETCDF_C_INCLUDE_DIRECTORIES=$NETCDFROOT/include
export NETCDF_CXX_INCLUDE_DIRECTORIES=/sw/rhel6-x64/netcdf/netcdf_cxx-4.2.1-gcc48/include
export LAPACK_LIB='-mkl=sequential'
export LAPACK_LIB_DEFAULT='-L/sw/rhel6-x64/intel/intel-18.0.1/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential'
export PERL5LIB=/usr/lib64/perl5
export SZIPROOT=/sw/rhel6-x64/sys/libaec-0.3.2-gcc48
export ZLIBROOT=/usr
export OASIS3MCT_FC_LIB="-L$NETCDFFROOT/lib -lnetcdff"
export LD_LIBRARY_PATH=$NETCDFROOT/lib:$NETCDFFROOT/lib:$HDF5ROOT/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/sw/rhel6-x64/gcc/gcc-4.8.2/lib64:$LD_LIBRARY_PATH
export I_MPI_FABRICS=shm:dapl
export I_MPI_FALLBACK=disable
export I_MPI_SLURM_EXT=1
export I_MPI_LARGE_SCALE_THRESHOLD=8192
export I_MPI_DYNAMIC_CONNECTION=1
export DAPL_NETWORK_NODES=$SLURM_NNODES
export DAPL_NETWORK_PPN=$SLURM_NTASKS_PER_NODE
export DAPL_WR_MAX=500
export OMPI_MCA_pml=cm
export OMPI_MCA_mtl=mxm
export OMPI_MCA_coll=^ghc
export OMPI_MCA_mtl_mxm_np=0
export MXM_RDMA_PORTS=mlx5_0:1
export MXM_LOG_LEVEL=FATAL
export GRIBAPIROOT=/sw/rhel6-x64/grib_api/grib_api-1.15.0-intel14
export PATH=$GRIBAPIROOT/bin:${PATH}
export UDUNITS2_ROOT=/sw/rhel6-x64/util/udunits-2.2.26-gcc64
export FFTW_ROOT=/sw/rhel6-x64/numerics/fftw-3.3.7-openmp-gcc64
export PROJ4_ROOT=/sw/rhel6-x64/graphics/proj4-4.9.3-gcc48
export PATH=/sw/rhel6-x64/gcc/binutils-2.24-gccsys/bin:${PATH}
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$GRIBAPIROOT/lib:$PROJ4_ROOT/lib:$FFTW_ROOT/lib
export OIFS_FFIXED=""
export GRIB_SAMPLES_PATH="$GRIBAPIROOT/share/grib_api/ifs_samples/grib1_mlgrib2/"
export DR_HOOK_IGNORE_SIGNALS=-1
export MPI_BUFS_PER_PROC=256
export OMP_SCHEDULE=STATIC
export OMP_STACKSIZE=128M
export ACCOUNT=ab0995
export ESM_TESTING_DIR=/work/ab0995/a270152/testing//run/awicm3/
export MODEL_DIR=/work/ab0995/a270152/testing//comp/awicm3/awicm3-v3.0
export FESOM_USE_CPLNG="active"
export ECE_CPL_NEMO_LIM="false"
export ECE_CPL_FESOM_FESIM="true"
export ECE_AWI_CPL_FESOM="true"
export ENVIRONMENT_SET_BY_ESMTOOLS=TRUE

unset SLURM_DISTRIBUTION
unset SLURM_NTASKS
unset SLURM_NPROCS
unset SLURM_ARBITRARY_NODELIST

# Set stack size to unlimited
ulimit -s unlimited
# 3...2...1...Liftoff!

echo $(date +"%a %b  %e %T %Y") : compute 1 2000-01-01T00:00:00 9719 - start >> /work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log//awicm3-v3.0-TCO159L91-CORE2_initial_awicm3.log

cd /work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/run_20000101-20000101/work/

#Creating hostlist for MPI + MPI&OMP heterogeneous parallel job
rm -f ./hostlist
export SLURM_HOSTFILE=/work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/run_20000101-20000101/work//hostlist
IFS=$'\n'; set -f
listnodes=($(< <( scontrol show hostnames $SLURM_JOB_NODELIST )))
unset IFS; set +f
rank=0
current_core=0
current_core_mpi=0
mpi_tasks_fesom=144
omp_threads_fesom=1
mpi_tasks_oifs=72
omp_threads_oifs=2
mpi_tasks_rnfmap=1
omp_threads_rnfmap=24
for model in fesom oifs rnfmap oasis3mct ;do
    eval nb_of_cores=\${mpi_tasks_${model}}
    eval nb_of_cores=$((${nb_of_cores}-1))
    for nb_proc_mpi in `seq 0 ${nb_of_cores}`; do
        (( index_host = current_core / 24 ))
        host_value=${listnodes[${index_host}]}
        (( slot =  current_core % 24 ))
        echo $host_value >> hostlist
        (( current_core = current_core + omp_threads_${model} ))
    done
done

time srun -l --kill-on-bad-exit=1 --cpu_bind=none --multi-prog hostfile_srun 2>&1 &

# Call to esm_runscript to start subjobs:
# ['tidy']
process=$! 
# Comment the following line if you don't want esm_runscripts to restart:
cd /work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/scripts/
esm_runscripts awicm3-v3.0-TCO159L91-CORE2_initial.yaml -e awicm3-v3.0-TCO159L91-CORE2_initial -t observe_compute -p ${process} -s 20000101 -r 1 -v  --open-run

echo $(date +"%a %b  %e %T %Y") : compute 1 2000-01-01T00:00:00 9719 - done >> /work/ab0995/a270152/testing//run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log//awicm3-v3.0-TCO159L91-CORE2_initial_awicm3.log

wait
