#!/bin/bash
#SBATCH --partition=compute
#SBATCH --time=00:45:00
#SBATCH --nodes=4
#SBATCH --output=<TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log/awicm3-v3.0-TCO159L91-CORE2_initial_awicm3_compute_20000101-20000101_%j.log --error=<TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log/awicm3-v3.0-TCO159L91-CORE2_initial_awicm3_compute_20000101-20000101_%j.log
#SBATCH --job-name=awicm3-v3.0-TCO159L91-CORE2_initial
#SBATCH --account=None
#SBATCH --mail-type=NONE
#SBATCH --exclusive

module purge
module load git/2.31.1-gcc-11.2.0
module load cdo/2.0.5-gcc-11.2.0
module load nco/5.0.6-gcc-11.2.0
module load intel-oneapi-compilers/2022.0.1-gcc-11.2.0
module load intel-oneapi-mkl/2022.0.1-gcc-11.2.0
module load openmpi/4.1.2-intel-2021.5.0
module load netcdf-c/4.8.1-openmpi-4.1.2-intel-2021.5.0
module load netcdf-fortran/4.5.3-openmpi-4.1.2-intel-2021.5.0
module load hdf5/1.12.1-openmpi-4.1.2-intel-2021.5.0

export LC_ALL=en_US.UTF-8
export CPU_MODEL=AMD_EPYC_ZEN3
export FESOM_PLATFORM_STRATEGY=levante.dkrz.de
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi2.so
export I_MPI_PMI=pmi2
export FI_PROVIDER=mlx
export I_MPI_OFI_PROVIDER=mlx
export I_MPI_FABRICS=shm:ofi
export HCOLL_ENABLE_MCAST_ALL=1
export HCOLL_MAIN_IB=mlx5_0:1
export UCX_IB_ADDR_TYPE=ib_global
export UCX_NET_DEVICES=mlx5_0:1
export UCX_TLS=mm,knem,cma,dc_mlx5,dc_x,self
export UCX_UNIFIED_MODE=y
export KMP_LIBRARY=turnaround
export KMP_AFFINITY=granularity=fine,scatter
export OMPI_MCA_btl=self
export OMPI_MCA_coll=^ml
export OMPI_MCA_coll_hcoll_enable=1
export OMPI_MCA_io=romio321
export OMPI_MCA_osc=pt2pt
export OMPI_MCA_pml=ucx
export FC=mpif90
export F77=mpif90
export MPICC=mpicc
export MPIFC=mpif90
export CC=mpicc
export CXX=mpicxx
export MPIROOT="$(mpif90 -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
export MPI_LIB="$(mpif90 -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"
export IO_LIB_ROOT=/work/ab0246/HPC_libraries/intel-oneapi-compilers/2022.0.1-gcc-11.2.0/openmpi/4.1.2-intel-2021.5.0
export HDF5ROOT=/sw/spack-levante/hdf5-1.12.1-tvymb5
export HDF5_C_INCLUDE_DIRECTORIES=$HDF5_ROOT/include
export HDF5_ROOT=$HDF5ROOT
export NETCDFFROOT=/sw/spack-levante/netcdf-fortran-4.5.3-k6xq5g
export NETCDFROOT=/sw/spack-levante/netcdf-c-4.8.1-2k3cmu
export NETCDF_Fortran_INCLUDE_DIRECTORIES=$NETCDFFROOT/include
export NETCDF_C_INCLUDE_DIRECTORIES=$NETCDFROOT/include
export NETCDF_CXX_INCLUDE_DIRECTORIES=$NETCDFROOT/include
export NETCDF_CXX_LIBRARIES=$NETCDFROOT/lib
export PSMPIFLAGS="-lrt -lm -ldl"
export LAPACK_LIB='-mkl=sequential'
export ZLIBROOT=/usr
export OASIS3MCT_FC_LIB="-L$NETCDFFROOT/lib -lnetcdff"
export PERL5LIB=/usr/lib64/perl5
export LD_LIBRARY_PATH=$HDF5ROOT/lib:$NETCDFROOT/lib:$NETCDFFROOT/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/sw/spack-levante/intel-oneapi-mkl-2022.0.1-ttdktf/mkl/2022.0.1/lib/intel64:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/sw/spack-levante/intel-oneapi-mpi-2021.5.0-mrcss7/mpi/2021.5.0/libfabric/lib:$LD_LIBRARY_PATH
export SZIPROOT=/sw/spack-levante/libaec-1.0.5-gij7yv
export ECCODESROOT=/work/ab0246/HPC_libraries/intel-oneapi-compilers/2022.0.1-gcc-11.2.0/openmpi/4.1.2-intel-2021.5.0
export PATH=$ECCODESROOT/bin:$PATH
export LD_LIBRARY_PATH=$ECCODESROOT/lib:$LD_LIBRARY_PATH
export OIFS_FFIXED=""
export GRIB_SAMPLES_PATH="$ECCODESROOT/share/eccodes/ifs_samples/grib1_mlgrib2"
export DR_HOOK_IGNORE_SIGNALS=-1
export OMP_SCHEDULE=STATIC
export OMP_STACKSIZE=128M
export ACCOUNT=None
export ESM_TESTING_DIR=<TEST_DIR>/run/awicm3/
export MODEL_DIR=<TEST_DIR>/comp/awicm3/awicm3-v3.0
export FESOM_USE_CPLNG="active"
export ECE_CPL_NEMO_LIM="false"
export ECE_CPL_FESOM_FESIM="true"
export ECE_AWI_CPL_FESOM="true"
export ENVIRONMENT_SET_BY_ESMTOOLS=TRUE

unset SLURM_DISTRIBUTION
unset SLURM_NTASKS
unset SLURM_NPROCS
unset SLURM_ARBITRARY_NODELIST

# Set stack size to unlimited
ulimit -s unlimited
# 3...2...1...Liftoff!

echo $(date +"%a %b  %e %T %Y") : compute 1 2000-01-01T00:00:00 1259 - start >> <TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log//awicm3-v3.0-TCO159L91-CORE2_initial_awicm3.log

cd <TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/run_20000101-20000101/work/

#Creating hostlist for MPI + MPI&OMP heterogeneous parallel job
rm -f ./hostlist
export SLURM_HOSTFILE=<TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/run_20000101-20000101/work//hostlist
IFS=$'\n'; set -f
listnodes=($(< <( scontrol show hostnames $SLURM_JOB_NODELIST )))
unset IFS; set +f
rank=0
current_core=0
current_core_mpi=0
mpi_tasks_fesom=128
omp_threads_fesom=1
mpi_tasks_oifs=128
omp_threads_oifs=2
mpi_tasks_rnfmap=1
omp_threads_rnfmap=128
for model in fesom oifs rnfmap oasis3mct ;do
    eval nb_of_cores=\${mpi_tasks_${model}}
    eval nb_of_cores=$((${nb_of_cores}-1))
    for nb_proc_mpi in `seq 0 ${nb_of_cores}`; do
        (( index_host = current_core / 128 ))
        host_value=${listnodes[${index_host}]}
        (( slot =  current_core % 128 ))
        echo $host_value >> hostlist
        (( current_core = current_core + omp_threads_${model} ))
    done
done

time srun -l --hint=nomultithread --multi-prog hostfile_srun 2>&1 &

# Call to esm_runscript to start subjobs:
# ['tidy']
process=$! 
# Comment the following line if you don't want esm_runscripts to restart:
cd <TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/scripts/
esm_runscripts awicm3-v3.0-TCO159L91-CORE2_initial.yaml -e awicm3-v3.0-TCO159L91-CORE2_initial -t observe_compute -p ${process} -s 20000101 -r 1 -v  --last-jobtype prepcompute --open-run

echo $(date +"%a %b  %e %T %Y") : compute 1 2000-01-01T00:00:00 1259 - done >> <TEST_DIR>/run/awicm3//awicm3-v3.0-TCO159L91-CORE2_initial/log//awicm3-v3.0-TCO159L91-CORE2_initial_awicm3.log

wait
